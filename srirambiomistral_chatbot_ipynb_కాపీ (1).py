# -*- coding: utf-8 -*-
"""sriramBioMistral_chatbot.ipynb కాపీ

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15TaLlwTS9vFDfkdqa7mbIIQlZUFnrzdP
"""

pip install -U openai-whisper

"""BlenderBot-Encoder-**Decoder**

"""

pip install --upgrade --no-deps --force-reinstall git+https://github.com/openai/whisper.git

pip install git+https://github.com/openai/whisper.git

!pip install langchain

!pip install langchain_community # Install the missing langchain_community package.

from langchain.llms import HuggingFacePipeline
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline

# Define model id
model_id = 'facebook/blenderbot-1B-distill'

# Load tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForSeq2SeqLM.from_pretrained(model_id)

# Create a pipeline
pipe = pipeline(
    "text2text-generation",
    model=model,
    tokenizer=tokenizer,
    max_length=100
)

# Initialize local LLM with HuggingFacePipeline
local_llm = HuggingFacePipeline(pipeline=pipe)

# Define the prompt
prompt = PromptTemplate(template="{question}", input_variables=["question"])

# Create the LLM chain
llm_chain = LLMChain(prompt=prompt, llm=local_llm)

# Define the question
question = "Tell me about you?"

# Run the chain with the question and print the result
print(llm_chain.run(question))

from langchain.llms import HuggingFacePipeline
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline

# Define model id
model_id = 'nikhil928/facebook-blenderbot-1B-distill'

# Load tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForSeq2SeqLM.from_pretrained(model_id)

# Create a pipeline
pipe = pipeline(
    "text2text-generation",
    model=model,
    tokenizer=tokenizer,
    max_length=100
)

# Initialize local LLM with HuggingFacePipeline
local_llm = HuggingFacePipeline(pipeline=pipe)

# Define the prompt template
prompt = PromptTemplate(template="{question}", input_variables=["question"])

# Create the LLM chain
llm_chain = LLMChain(prompt=prompt, llm=local_llm)

# Start the interactive loop
while True:
    # Get user input
    question = input("Ask a question (or type 'exit' to quit): ")

    # Check if the user wants to exit
    if question.lower() == 'exit':
        print("Exiting the conversation.")
        break

    # Run the chain with the question and print the result
    response = llm_chain.run(question)
    print(f"Response: {response}\n")

"""# Build BioMistral Medical RAG Chatbot using BioMistral Open Source LLM

In the notebook we will build a Medical Chatbot with BioMistral LLM and Heart Health pdf file.
"""

#shell
!pip install huggingface_hub

from transformers import pipeline

# Initialize the pipeline with the correct model
pipe = pipeline("text-generation", model="Qwen/Qwen2-VL-7B-Instruct")

# Text input
input_text = "Who are you?"

# Generate text with the given input and max length
result = pipe(input_text, max_length=50, num_return_sequences=1, do_sample=True)

# Print the result
for i, generated_text in enumerate(result):
    print(f"Generated Text {i + 1}: {generated_text['generated_text']}")

from transformers import pipeline

# Initialize the pipeline with the correct model
pipe = pipeline("text-generation", model="Qwen/Qwen2-VL-7B-Instruct")

# Text input
input_text = "Who are you?"

# Generate text with the given input and max length
result = pipe(input_text, max_length=50, num_return_sequences=1, do_sample=True)

# Print the result
for i, generated_text in enumerate(result):
    print(f"Generated Text {i + 1}: {generated_text['generated_text']}")

"""## Installation"""

!pip install langchain sentence-transformers chromadb llama-cpp-python langchain_community pypdf

"""## Import libraries"""

from langchain_community.document_loaders import PyPDFDirectoryLoader
from langchain.text_splitter import CharacterTextSplitter,RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS, Chroma
from langchain_community.llms import LlamaCpp
from langchain.chains import RetrievalQA, LLMChain

import pathlib
import textwrap
from IPython.display import display
from IPython.display import Markdown



def to_markdown(text):
  text = text.replace('•', '  *')
  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))

# Used to securely store your API key
from google.colab import userdata

"""## Setup HuggingFace Access Token

- Log in to [HuggingFace.co](https://huggingface.co/)
- Click on your profile icon at the top-right corner, then choose [“Settings.”](https://huggingface.co/settings/)
- In the left sidebar, navigate to [“Access Token”](https://huggingface.co/settings/tokens)
- Generate a new access token, assigning it the “write” role.

"""

# Or use `os.getenv('HUGGINGFACEHUB_API_TOKEN')` to fetch an environment variable.
import os
from getpass import getpass

HUGGINGFACEHUB_API_TOKEN = userdata.get("HUGGINGFACEHUB_API_TOKEN")
os.environ["HUGGINGFACEHUB_API_TOKEN"] = "HUGGINGFACEHUB_API_TOKEN"

"""## Import document"""

#connect to google drive
from google.colab import drive
drive.mount('/content/drive') #Fixed typo in directory name

loader = PyPDFDirectoryLoader("/content/drive/MyDrive/Pdf")
docs = loader.load()

docs

"""## Text Splitting - Chunking"""

text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
chunks = text_splitter.split_documents(docs)

len(chunks)

chunks[0]

chunks[1]

chunks[2]

chunks[3]

chunks[4]

embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-base-en-v1.5")

"""## Embeddings

## Vector Store - FAISS or ChromaDB
"""

vectorstore = Chroma.from_documents(chunks, embeddings)

vectorstore

query = "Result" # what is at risk of heart disease
search = vectorstore.similarity_search(query)

to_markdown(search[0].page_content)

"""## Retriever"""

retriever = vectorstore.as_retriever(
    search_kwargs={'k': 5}
)

retriever.get_relevant_documents(query)

"""## Large Language Model - Open Source"""

drive.mount('/content/drive')

llm = LlamaCpp(
    model_path= "/content/drive/MyDrive/Pdf/Bio/BioMistral-7B.Q4_K_M.gguf",
    temperature=0.3,
    max_tokens=2048,
    top_p=1)

"""## RAG Chain"""

from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from langchain.prompts import ChatPromptTemplate

template = """
<|context|>
You are an AI assistant that follows instruction extremely well.
Please be truthful and give direct answers
</s>
<|user|>
{query}
</s>
 <|assistant|>
"""

prompt = ChatPromptTemplate.from_template(template)

rag_chain = (
    {"context": retriever,  "query": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)



response = rag_chain.invoke("what disease ?")

to_markdown(response)

import sys

while True:
  user_input = input(f"Input Prompt: ")
  if user_input == 'exit':
    print('Exiting')
    sys.exit()
  if user_input == '':
    continue
  result = rag_chain.invoke(user_input)
  print("Answer: ",result)

!pip install pyttsx3

import pyttsx3
import sys
from langchain_core._api.deprecation import surface_langchain_deprecation_warnings

# Initialize text-to-speech engine
engine = pyttsx3.init()

# Function to convert text to speech
def speak(text):
    engine.say(text)
    engine.runAndWait()

# Your existing code
from langchain_community.document_loaders import PyPDFDirectoryLoader
from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS, Chroma
from langchain_community.llms import LlamaCpp
from langchain.chains import RetrievalQA, LLMChain

import pathlib
import textwrap
from IPython.display import display, Markdown

def to_markdown(text):
    text = text.replace('•', '  *')
    return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))

# Setup HuggingFace Access Token
import os
from getpass import getpass

HUGGINGFACEHUB_API_TOKEN = os.getenv("HUGGINGFACEHUB_API_TOKEN")
os.environ["HUGGINGFACEHUB_API_TOKEN"] = HUGGINGFACEHUB_API_TOKEN

# Import document
from google.colab import drive
drive.mount('/content/drive')

loader = PyPDFDirectoryLoader("/content/drive/MyDrive/Pdf")
docs = loader.load()

# Text Splitting - Chunking
text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
chunks = text_splitter.split_documents(docs)

embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-base-en-v1.5")

# Vector Store - FAISS or ChromaDB
vectorstore = Chroma.from_documents(chunks, embeddings)

query = "Result"
search = vectorstore.similarity_search(query)
to_markdown(search[0].page_content)

# Retriever
retriever = vectorstore.as_retriever(search_kwargs={'k': 5})
retriever.get_relevant_documents(query)

# Large Language Model - Open Source
llm = LlamaCpp(
    model_path= "/content/drive/MyDrive/Pdf/Bio/BioMistral-7B.Q4_K_M.gguf",
    temperature=0.3,
    max_tokens=2048,
    top_p=1
)

# RAG Chain
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from langchain.prompts import ChatPromptTemplate

template = """
<|context|>
You are an AI assistant that follows instruction extremely well.
Please be truthful and give direct answers
</s>
<|user|>
{query}
</s>
 <|assistant|>
"""

prompt = ChatPromptTemplate.from_template(template)
rag_chain = (
    {"context": retriever, "query": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# Interactive Loop
while True:
    user_input = input(f"Input Prompt: ")
    if user_input == 'exit':
        print('Exiting')
        speak('Exiting')
        sys.exit()
    if user_input == '':
        continue
    result = rag_chain.invoke(user_input)
    print("Answer: ", result)
    speak(result)

!sudo apt-get update
!sudo apt-get install espeak

!sudo apt-get install libespeak1

engine = pyttsx3.init(driverName='espeak')

import pyttsx3
import sys
from langchain_core._api.deprecation import surface_langchain_deprecation_warnings

# Initialize text-to-speech engine, explicitly specifying the espeak driver
engine = pyttsx3.init(driverName='espeak')

# Function to convert text to speech
def speak(text):
    engine.say(text)
    engine.runAndWait()

# Your existing code
from langchain_community.document_loaders import PyPDFDirectoryLoader
from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS, Chroma
from langchain_community.llms import LlamaCpp
from langchain.chains import RetrievalQA, LLMChain

import pathlib
import textwrap
from IPython.display import display, Markdown

def to_markdown(text):
    text = text.replace('•', '  *')
    return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))

# Setup HuggingFace Access Token
import os
from getpass import getpass

HUGGINGFACEHUB_API_TOKEN = os.getenv("HUGGINGFACEHUB_API_TOKEN")
os.environ["HUGGINGFACEHUB_API_TOKEN"] = HUGGINGFACEHUB_API_TOKEN

# Import document
from google.colab import drive
drive.mount('/content/drive')

loader = PyPDFDirectoryLoader("/content/drive/MyDrive/Pdf")
docs = loader.load()

# Text Splitting - Chunking
text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
chunks = text_splitter.split_documents(docs)

embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-base-en-v1.5")

# Vector Store - FAISS or ChromaDB
vectorstore = Chroma.from_documents(chunks, embeddings)

query = "Result"
search = vectorstore.similarity_search(query)
to_markdown(search[0].page_content)

# Retriever
retriever = vectorstore.as_retriever(search_kwargs={'k': 5})
retriever.get_relevant_documents(query)

# Large Language Model - Open Source
llm = LlamaCpp(
    model_path= "/content/drive/MyDrive/Pdf/Bio/BioMistral-7B.Q4_K_M.gguf",
    temperature=0.3,
    max_tokens=2048,
    top_p=1
)

# RAG Chain
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from langchain.prompts import ChatPromptTemplate

template = """
<|context|>
You are an AI assistant that follows instruction extremely well.
Please be truthful and give direct answers
</s>
<|user|>
{query}
</s>
 <|assistant|>
"""

prompt = ChatPromptTemplate.from_template(template)
rag_chain = (
    {"context": retriever, "query": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# Interactive Loop
while True:
    user_input = input(f"Input Prompt: ")
    if user_input == 'exit':
        print('Exiting')
        speak('Exiting')
        sys.exit()
    if user_input == '':
        continue
    result = rag_chain.invoke(user_input)
    print("Answer: ", result)
    speak(result)