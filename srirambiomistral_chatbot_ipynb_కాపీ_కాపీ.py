# -*- coding: utf-8 -*-
"""sriramBioMistral_chatbot.ipynb కాపీ కాపీ

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cf-vAZZVRWefFbkpQmyNlrfOeKiy_Mw0

# Build BioMistral Medical RAG Chatbot using BioMistral Open Source LLM

In the notebook we will build a Medical Chatbot with BioMistral LLM and Heart Health pdf file.

## Installation
"""

!pip install langchain sentence-transformers chromadb llama-cpp-python langchain_community pypdf

"""## Import libraries"""

from langchain_community.document_loaders import PyPDFDirectoryLoader
from langchain.text_splitter import CharacterTextSplitter,RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS, Chroma
from langchain_community.llms import LlamaCpp
from langchain.chains import RetrievalQA, LLMChain

import pathlib
import textwrap
from IPython.display import display
from IPython.display import Markdown



def to_markdown(text):
  text = text.replace('•', '  *')
  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))

# Used to securely store your API key
from google.colab import userdata

"""## Setup HuggingFace Access Token

- Log in to [HuggingFace.co](https://huggingface.co/)
- Click on your profile icon at the top-right corner, then choose [“Settings.”](https://huggingface.co/settings/)
- In the left sidebar, navigate to [“Access Token”](https://huggingface.co/settings/tokens)
- Generate a new access token, assigning it the “write” role.

"""

# Or use `os.getenv('HUGGINGFACEHUB_API_TOKEN')` to fetch an environment variable.
import os
from getpass import getpass

HUGGINGFACEHUB_API_TOKEN = userdata.get("HUGGINGFACEHUB_API_TOKEN")
os.environ["HUGGINGFACEHUB_API_TOKEN"] = "HUGGINGFACEHUB_API_TOKEN"

"""## Import document"""

#connect to google drive
from google.colab import drive
drive.mount('/content/drive') #Fixed typo in directory name

loader = PyPDFDirectoryLoader("/content/drive/MyDrive/Pdf")
docs = loader.load()

docs

"""## Text Splitting - Chunking"""

text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
chunks = text_splitter.split_documents(docs)

len(chunks)

chunks[0]

chunks[1]

chunks[2]

chunks[3]

chunks[4]

"""## Embeddings"""

embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-base-en-v1.5")

"""## Vector Store - FAISS or ChromaDB"""

vectorstore = Chroma.from_documents(chunks, embeddings)

vectorstore

query = "what is network" # what is at risk of heart disease
search = vectorstore.similarity_search(query)

to_markdown(search[0].page_content)

"""## Retriever"""

retriever = vectorstore.as_retriever(
    search_kwargs={'k': 5}
)

retriever.get_relevant_documents(query)

"""## Large Language Model - Open Source"""

drive.mount('/content/drive')

llm = LlamaCpp(
    model_path= "/content/drive/MyDrive/Pdf/Bio/BioMistral-7B.Q4_K_M.gguf",
    temperature=0.3,
    max_tokens=2048,
    top_p=1)

"""## RAG Chain"""

from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from langchain.prompts import ChatPromptTemplate

template = """
<|context|>
You are an AI assistant that follows instruction extremely well.
Please be truthful and give direct answers
</s>
<|user|>
{query}
</s>
 <|assistant|>
"""

prompt = ChatPromptTemplate.from_template(template)

rag_chain = (
    {"context": retriever,  "query": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

response = rag_chain.invoke("what disease affect the heart?")

to_markdown(response)

import sys

while True:
  user_input = input(f"Input Prompt: ")
  if user_input == 'exit':
    print('Exiting')
    sys.exit()
  if user_input == '':
    continue
  result = rag_chain.invoke(user_input)
  print("Answer: ",result)